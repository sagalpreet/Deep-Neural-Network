{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0NSekBxgElmd2T5YVGJNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagalpreet/Deep-Neural-Network/blob/master/psinet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPCSzGXo8hGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''\n",
        "Things to add later include adding asserts in proper places to avoid user from invalid inputs\n",
        "'''\n",
        "\n",
        "# number of layer includes both the input layer and the outer layer.\n",
        "\n",
        "class psinet:\n",
        "    \n",
        "    # initializing the neural network involves defining its architecture in the form of \n",
        "    # number of layers and number of nodes in each of them\n",
        "    # 0th index in list_of_number_of_nodes_in_each_layer denotes input layer\n",
        "    # dictionary to contain the values of parameters is also defined\n",
        "    \n",
        "    def __init__(self, number_of_layers, list_of_number_of_nodes_in_each_layer, act, lf):\n",
        "        # act denotes list_of_activation_functions_to_be_applied - from layer one to output layer although indexing goes as\n",
        "        # usual lists of python i.e from 0\n",
        "        # lf denotes the loss function\n",
        "        self.num_l = number_of_layers\n",
        "        self.num_n = list_of_number_of_nodes_in_each_layer\n",
        "        self.parameters = {}\n",
        "        self.a = {}\n",
        "        self.act = act\n",
        "        self.lf = lf\n",
        "        \n",
        "        # initializing parameters\n",
        "        \n",
        "        for i in range(1, self.num_l):\n",
        "            self.parameters['w'+str(i)] = np.random.random((self.num_n[i], self.num_n[i-1]))\n",
        "            self.parameters['b'+str(i)] = np.random.random((self.num_n[i], 1))\n",
        "        \n",
        "        # values to be stored at each layer - this will be initialized during training as the number of training examples to\n",
        "        # be sent in a batch remains an attribute which when taken at the time of training makes much more sense\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # input and output array are expected to be given as input\n",
        "    \n",
        "    def train(self, x, y, alpha):\n",
        "                \n",
        "        num_training_examples = x.shape[1]\n",
        "        \n",
        "        # initializing the values to be stored at each layer i.e 'a' in notation\n",
        "        # i = 0 in the following loop corresponds to input layer denoted by a[0] \n",
        "        \n",
        "        for i in range(self.num_l):\n",
        "            self.a[i] = np.random.random((self.num_n[i], num_training_examples))\n",
        "        \n",
        "        self.a[0] = x\n",
        "        \n",
        "        \n",
        "        # forward propagation\n",
        "        \n",
        "        for i in range(1, self.num_l):\n",
        "            # A is the function which returns the activated value of z given the type of activation function and z as input\n",
        "            \n",
        "            self.a[i] =  A ( self.act[i-1], ( ( self.parameters['w'+str(i)] @ self.a[i-1] ) + self.parameters['b'+str(i)] ) )\n",
        "        \n",
        "        \n",
        "        # back propagation\n",
        "        \n",
        "        # In theory we understood backpropagation as recursion with last layer derivative as base case.\n",
        "        # We follow the same paradigm here.\n",
        "        \n",
        "        # we use da as a variable and alter its value at each step as storing it is not required\n",
        "        \n",
        "        # base case\n",
        "        \n",
        "        # dC is a function that returns the partial derivative of the cost function wrt to the nodes in the output layer.\n",
        "        da = dC (self.lf, self.a[self.num_l-1], y)\n",
        "        \n",
        "        # recursive step\n",
        "        \n",
        "        for i in range(self.num_l-1, 0, -1):\n",
        "            Ia = I(self.act[i-1], self.a[i]) # derivative of a wrt z in terms of a, nd array of same dimensions as that of a\n",
        "            dz = da * Ia\n",
        "            db = np.sum(dz,axis=1,keepdims=True)\n",
        "            dw = dz @ (self.a[i-1].T)\n",
        "            \n",
        "            # gradient descent\n",
        "            self.parameters[\"w\"+str(i)] = self.parameters[\"w\"+str(i)] - ( alpha * (dw) ) \n",
        "            self.parameters[\"b\"+str(i)] = self.parameters[\"b\"+str(i)] - ( alpha * (db) )\n",
        "            \n",
        "            da = (self.parameters[\"w\"+str(i)].T) @ dz\n",
        "            \n",
        "        # computing cost\n",
        "        \n",
        "        cost = C (self.lf, self.a[self.num_l-1], y)\n",
        "        \n",
        "        return cost\n",
        "        \n",
        "    \n",
        "    def evaluate(self, x):\n",
        "        \n",
        "        num_training_examples = x.shape[1]\n",
        "        \n",
        "        # initializing the values to be stored at each layer i.e 'a' in notation\n",
        "        # i = 0 in the following loop corresponds to input layer denoted by a[0] \n",
        "        \n",
        "        for i in range(self.num_l):\n",
        "            self.a[i] = np.random.random((self.num_n[i], num_training_examples))\n",
        "        \n",
        "        self.a[0] = x\n",
        "        \n",
        "        # forward propagation\n",
        "        \n",
        "        for i in range(1, self.num_l):\n",
        "            # A is the function which returns the activated value of z given the type of activation function and z as input\n",
        "            \n",
        "            self.a[i] =  A ( self.act[i-1], ( ( self.parameters['w'+str(i)] @ self.a[i-1] ) + self.parameters['b'+str(i)] ) )\n",
        "              \n",
        "        return self.a[self.num_l-1]\n",
        "    \n",
        "    def test(self, x, y):\n",
        "        \n",
        "        predicted = self.evaluate(x)\n",
        "        \n",
        "        # computing cost\n",
        "        cost = C (self.lf, predicted, y)\n",
        "        print(\"Cost: \",cost)\n",
        "        \n",
        "        # computing accuracy\n",
        "        accuracy = 100*(np.sum(predicted==y))/y.size\n",
        "        \n",
        "\n",
        "\n",
        "# A is the function which returns the activated value of z given the type of activation function and z as input\n",
        "def A(f, z):\n",
        "    if f==\"sigmoid\" :\n",
        "        return 1/(1+np.exp(-z))\n",
        "    if f==\"relu\" :\n",
        "        return (z+np.abs(z))/2\n",
        "        \n",
        "\n",
        "# dC is a function that returns the partial derivative of the cost function wrt to the nodes in the output layer.\n",
        "def dC(lf, yhat, y):\n",
        "    if lf==\"mse\" : # mean squared error\n",
        "        return 2*(yhat-y)\n",
        "    if lf==\"cel\" : # cross entropy loss\n",
        "        return (yhat-y)/(yhat*(1-yhat))\n",
        "\n",
        "\n",
        "def C(lf, yhat, y):\n",
        "    if lf==\"mse\" : # mean squared error\n",
        "        return np.sum( np.square(yhat-y) ,axis = 1, keepdims = True)\n",
        "    if lf==\"cel\" : # cross entropy loss\n",
        "        return np.sum( -y*np.log(yhat)-(1-y)*np.log(1-yhat) ,axis = 1, keepdims = True)\n",
        "    \n",
        "\n",
        "# derivative of a wrt z in terms of a, nd array of same dimensions as that of a\n",
        "def I(f, a):\n",
        "    if f==\"sigmoid\" :\n",
        "        return a*(1-a)\n",
        "    if f==\"relu\" :\n",
        "        return 1*(a>0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pccwXbE8iCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}